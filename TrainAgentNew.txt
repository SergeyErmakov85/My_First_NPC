{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение агента в Unity ML-Agents с экспортом ONNX\n",
    "\n",
    "Этот ноутбук обучает агента из Unity-среды (`MyAgent?team=0`) с 6 наблюдениями, 2 непрерывными действиями и 1 дискретным действием. Используем PPO из `stable-baselines3` для обучения и экспортируем модель в ONNX для Unity.\n",
    "\n",
    "## Зависимости\n",
    "- Установите необходимые библиотеки:\n",
    "  ```bash\n",
    "  pip install mlagents==0.30.0 stable-baselines3==2.0.0 gymnasium==0.29.1 torch==2.0.1 onnx numpy\n",
    "  ```\n",
    "- Убедитесь, что путь к `UnityEnvironment.exe` правильный.\n",
    "- Среда Unity должна быть собрана с `Behavior Name: MyAgent?team=0`.\n",
    "- Убедитесь, что Unity ML-Agents версия в проекте (например, 2.0.1) совместима с `mlagents==0.30.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт библиотек\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# Путь к среде Unity\n",
    "env_path = os.path.join(os.getcwd(), r'N:\\MyRL\\My_First_NPC\\MyfirstMPC\\UnityEnvironment.exe')  # Укажите свой путь\n",
    "\n",
    "# Настройка канала для ускорения симуляции\n",
    "engine_channel = EngineConfigurationChannel()\n",
    "engine_channel.set_configuration_parameters(time_scale=50.0, quality_level=0)\n",
    "\n",
    "# Инициализация среды\n",
    "env = UnityEnvironment(file_name=env_path, side_channels=[engine_channel])\n",
    "env.reset()\n",
    "\n",
    "# Получение имени поведения\n",
    "behavior_name = list(env.behavior_specs.keys())[0]\n",
    "print(f'Behavior Name: {behavior_name}')\n",
    "spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "# Проверка спецификации\n",
    "print(f'Observation size: {spec.observation_specs[0].shape[0]}')\n",
    "print(f'Continuous action size: {spec.action_spec.continuous_size}')\n",
    "print(f'Discrete action branches: {spec.action_spec.discrete_branches}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание кастомной нейросети\n",
    "\n",
    "Определяем нейросеть с двумя выходами: для непрерывных действий (2) и дискретного действия (1). Используем `BaseFeaturesExtractor` для совместимости с `stable-baselines3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кастомная нейросеть\n",
    "class CustomActorCriticNet(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=128):\n",
    "        super(CustomActorCriticNet, self).__init__(observation_space, features_dim)\n",
    "        self.fc1 = nn.Linear(observation_space.shape[0], 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, features_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Определяем политику для PPO\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomActorCriticNet,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    "    net_arch=[dict(pi=[64, 32], vf=[64, 32])]  # Архитектура для актора и критика\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обёртка среды для Gymnasium\n",
    "\n",
    "Создаём обёртку, совместимую с `gymnasium.Env`, для работы с `stable-baselines3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityGymWrapper(gym.Env):\n",
    "    def __init__(self, unity_env, behavior_name, spec):\n",
    "        super(UnityGymWrapper, self).__init__()\n",
    "        self.env = unity_env\n",
    "        self.behavior_name = behavior_name\n",
    "        self.spec = spec\n",
    "        \n",
    "        # Пространства наблюдений и действий\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(spec.observation_specs[0].shape[0],), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Если есть и непрерывные, и дискретные действия\n",
    "        self.action_space = spaces.Dict({\n",
    "            'continuous': spaces.Box(low=-1.0, high=1.0, shape=(spec.action_spec.continuous_size,), dtype=np.float32),\n",
    "            'discrete': spaces.MultiDiscrete(spec.action_spec.discrete_branches)\n",
    "        })\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.env.reset()\n",
    "        decision_steps, terminal_steps = self.env.get_steps(self.behavior_name)\n",
    "        obs = decision_steps.obs[0][0]  # Первое наблюдение\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Создаём ActionTuple\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_continuous(action['continuous'])\n",
    "        action_tuple.add_discrete(action['discrete'].reshape(-1, len(self.spec.action_spec.discrete_branches)))\n",
    "\n",
    "        self.env.set_actions(self.behavior_name, action_tuple)\n",
    "        self.env.step()\n",
    "\n",
    "        decision_steps, terminal_steps = self.env.get_steps(self.behavior_name)\n",
    "        done = len(terminal_steps) > 0\n",
    "        reward = terminal_steps.reward[0] if done else decision_steps.reward[0]\n",
    "        obs = terminal_steps.obs[0][0] if done else decision_steps.obs[0][0]\n",
    "        info = {}\n",
    "        truncated = False  # Unity сама управляет завершением эпизодов\n",
    "\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "# Создаём обёртку\n",
    "gym_env = UnityGymWrapper(env, behavior_name, spec)\n",
    "check_env(gym_env)  # Проверяем совместимость с Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели\n",
    "\n",
    "Используем PPO для обучения. Гиперпараметры подобраны для стабильного обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация PPO\n",
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    gym_env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Обучение на 100,000 шагов\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# Сохранение модели\n",
    "model.save('ppo_myagent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Экспорт модели в ONNX\n",
    "\n",
    "Экспортируем модель в формат ONNX, совместимый с Unity Barracuda. Учитываем имена входов/выходов и динамический батч."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем политику из модели\n",
    "policy = model.policy.to('cpu')  # Переводим на CPU для экспорта\n",
    "\n",
    "# Создаём фиктивный вход\n",
    "dummy_input = torch.randn(1, spec.observation_specs[0].shape[0])\n",
    "\n",
    "# Экспорт в ONNX\n",
    "torch.onnx.export(\n",
    "    policy,\n",
    "    dummy_input,\n",
    "    'trained_myagent.onnx',\n",
    "    input_names=['obs_0'],\n",
    "    output_names=['continuous_actions', 'discrete_actions'],\n",
    "    dynamic_axes={\n",
    "        'obs_0': {0: 'batch_size'},\n",
    "        'continuous_actions': {0: 'batch_size'},\n",
    "        'discrete_actions': {0: 'batch_size'}\n",
    "    },\n",
    "    opset_version=9,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print('Модель успешно сохранена: trained_myagent.onnx')\n",
    "\n",
    "# Проверка существования файла\n",
    "print('Файл существует:', os.path.exists('trained_myagent.onnx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование модели\n",
    "\n",
    "Проверяем, как обученная модель работает в среде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование\n",
    "obs, _ = gym_env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = gym_env.step(action)\n",
    "    if done or truncated:\n",
    "        print('Эпизод завершён')\n",
    "        obs, _ = gym_env.reset()\n",
    "\n",
    "# Закрытие среды\n",
    "gym_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}