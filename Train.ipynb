{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7813cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import os\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.registry import default_registry\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e4c57",
   "metadata": {},
   "source": [
    "# Обучение агента в Unity ML-Agents\n",
    "В этом ноутбуке показан пример запуска обучения агента в пользовательской среде Unity, расположенной в этой папке. Для этого используется пакет `mlagents` и Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d80c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доступные поведения: ['MyAgent?team=0']\n"
     ]
    }
   ],
   "source": [
    "# Путь к вашему Unity окружению (укажите путь к .exe, если экспортировано, либо используйте Editor для запуска из редактора)\n",
    "# env_path = None  # Если среда запускается из редактора Unity, иначе укажите путь к .exe\n",
    "\n",
    "env_path = os.path.join(os.getcwd(), r'N:\\MyRL\\MyfirstMPC\\UnityEnvironment.exe')  # Замените на ваш путь к .exe\n",
    "\n",
    "\n",
    "engine_channel = EngineConfigurationChannel()\n",
    "env = UnityEnvironment(file_name=env_path, side_channels=[engine_channel])\n",
    "env.reset()\n",
    "# Получение информации о среде\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "print('Доступные поведения:', behavior_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cf748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация среды Unity\n",
    "# env = UnityEnvironment(file_name=\"path/to/your/environment\")  # Укажите путь к вашему файлу среды\n",
    "# env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "801f4aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпизод 1 завершён.\n"
     ]
    }
   ],
   "source": [
    "# Пример цикла взаимодействия с агентом\n",
    "behavior_name = behavior_names[0]  # Используем первое поведение\n",
    "spec = env.behavior_specs[behavior_name]\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "for episode in range(1):\n",
    "    env.reset()\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    while len(terminal_steps) == 0:\n",
    "        action = spec.action_spec.random_action(len(decision_steps))\n",
    "        env.set_actions(behavior_name, action)\n",
    "        env.step()\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    print(f\"Эпизод {episode+1} завершён.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a648a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение имени поведения\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_name = behavior_names[0]\n",
    "spec = env.behavior_specs[behavior_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b441802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Имя поведения: MyAgent?team=0\n",
      "Спецификация поведения: BehaviorSpec(observation_specs=[ObservationSpec(shape=(6,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size6')], action_spec=ActionSpec(continuous_size=2, discrete_branches=(1,)))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Имя поведения: {behavior_name}\")\n",
    "# Получение спецификации поведения\n",
    "print(f\"Спецификация поведения: {spec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b633129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример простой нейросети для агента\n",
    "class SimpleAgentNet(nn.Module):\n",
    "    def __init__(self, obs_size, action_size):\n",
    "        super(SimpleAgentNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x может быть (obs_size,) или (batch, obs_size)\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fa27cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер наблюдений: 6\n",
      "Размер действий: 2\n",
      "Начинаем обучение на 100000 эпизодов...\n"
     ]
    }
   ],
   "source": [
    "# Получаем размеры входа и выхода из спецификации поведения\n",
    "obs_size = spec.observation_specs[0].shape[0]\n",
    "action_size = spec.action_spec.continuous_size\n",
    "\n",
    "# Инициализация сети, оптимизатора и функции потерь\n",
    "net = SimpleAgentNet(obs_size, action_size)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Гиперпараметры\n",
    "num_episodes = 100000\n",
    "gamma = 0.99\n",
    "\n",
    "print(f\"Размер наблюдений: {obs_size}\")\n",
    "print(f\"Размер действий: {action_size}\")\n",
    "print(f\"Начинаем обучение на {num_episodes} эпизодов...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a2cef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.close()  # Закрытие среды после завершения обучения"
   ]
  }
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "id": "training_process",
  "metadata": {},
  "outputs": [],
  "source": [
   "# Полный процесс обучения агента с сохранением модели в ONNX\n",
   "import torch\n",
   "import torch.nn as nn\n",
   "import torch.optim as optim\n",
   "import numpy as np\n",
   "import matplotlib.pyplot as plt\n",
   "from collections import deque\n",
   "import random\n",
   "from mlagents_envs.environment import UnityEnvironment\n",
   "from mlagents_envs.base_env import ActionTuple\n",
   "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
   "import os\n",
   "\n",
   "# Настройка устройства\n",
   "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
   "print(f\"Используется устройство: {device}\")\n",
   "\n",
   "# Улучшенная архитектура нейронной сети\n",
   "class ImprovedAgentNet(nn.Module):\n",
   "    def __init__(self, obs_size, action_size, hidden_size=256):\n",
   "        super(ImprovedAgentNet, self).__init__()\n",
   "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
   "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
   "        self.fc3 = nn.Linear(hidden_size, hidden_size // 2)\n",
   "        self.fc4 = nn.Linear(hidden_size // 2, action_size)\n",
   "        self.dropout = nn.Dropout(0.2)\n",
   "        \n",
   "    def forward(self, x):\n",
   "        if x.dim() == 1:\n",
   "            x = x.unsqueeze(0)\n",
   "        x = torch.relu(self.fc1(x))\n",
   "        x = self.dropout(x)\n",
   "        x = torch.relu(self.fc2(x))\n",
   "        x = self.dropout(x)\n",
   "        x = torch.relu(self.fc3(x))\n",
   "        x = torch.tanh(self.fc4(x))  # Ограничиваем выход в диапазоне [-1, 1]\n",
   "        return x\n",
   "\n",
   "# Класс для хранения опыта\n",
   "class ReplayBuffer:\n",
   "    def __init__(self, capacity):\n",
   "        self.buffer = deque(maxlen=capacity)\n",
   "    \n",
   "    def push(self, state, action, reward, next_state, done):\n",
   "        self.buffer.append((state, action, reward, next_state, done))\n",
   "    \n",
   "    def sample(self, batch_size):\n",
   "        batch = random.sample(self.buffer, batch_size)\n",
   "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
   "        return state, action, reward, next_state, done\n",
   "    \n",
   "    def __len__(self):\n",
   "        return len(self.buffer)\n",
   "\n",
   "# Инициализация среды\n",
   "env_path = os.path.join(os.getcwd(), r'N:\\MyRL\\MyfirstMPC\\UnityEnvironment.exe')\n",
   "engine_channel = EngineConfigurationChannel()\n",
   "env = UnityEnvironment(file_name=env_path, side_channels=[engine_channel])\n",
   "env.reset()\n",
   "\n",
   "# Получение информации о среде\n",
   "behavior_names = list(env.behavior_specs.keys())\n",
   "behavior_name = behavior_names[0]\n",
   "spec = env.behavior_specs[behavior_name]\n",
   "\n",
   "obs_size = spec.observation_specs[0].shape[0]\n",
   "action_size = spec.action_spec.continuous_size\n",
   "\n",
   "print(f\"Размер наблюдений: {obs_size}\")\n",
   "print(f\"Размер действий: {action_size}\")\n",
   "\n",
   "# Инициализация модели и оптимизатора\n",
   "model = ImprovedAgentNet(obs_size, action_size).to(device)\n",
   "target_model = ImprovedAgentNet(obs_size, action_size).to(device)\n",
   "target_model.load_state_dict(model.state_dict())\n",
   "\n",
   "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
   "criterion = nn.MSELoss()\n",
   "\n",
   "# Гиперпараметры\n",
   "num_episodes = 2000\n",
   "max_steps_per_episode = 1000\n",
   "batch_size = 64\n",
   "gamma = 0.99\n",
   "epsilon = 1.0\n",
   "epsilon_min = 0.01\n",
   "epsilon_decay = 0.995\n",
   "target_update_freq = 100\n",
   "buffer_size = 100000\n",
   "\n",
   "# Инициализация буфера опыта\n",
   "replay_buffer = ReplayBuffer(buffer_size)\n",
   "\n",
   "# Списки для отслеживания прогресса\n",
   "episode_rewards = []\n",
   "episode_lengths = []\n",
   "losses = []\n",
   "\n",
   "print(\"Начинаем обучение...\")\n",
   "\n",
   "for episode in range(num_episodes):\n",
   "    env.reset()\n",
   "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
   "    \n",
   "    episode_reward = 0\n",
   "    episode_length = 0\n",
   "    \n",
   "    # Получаем начальное состояние\n",
   "    if len(decision_steps) > 0:\n",
   "        state = decision_steps.obs[0][0]  # Первое наблюдение первого агента\n",
   "    else:\n",
   "        continue\n",
   "    \n",
   "    for step in range(max_steps_per_episode):\n",
   "        # Выбор действия (epsilon-greedy)\n",
   "        if random.random() < epsilon:\n",
   "            # Случайное действие\n",
   "            action = np.random.uniform(-1, 1, action_size)\n",
   "        else:\n",
   "            # Действие от модели\n",
   "            with torch.no_grad():\n",
   "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
   "                action = model(state_tensor).cpu().numpy()[0]\n",
   "        \n",
   "        # Выполнение действия\n",
   "        action_tuple = ActionTuple(continuous=np.array([action]))\n",
   "        env.set_actions(behavior_name, action_tuple)\n",
   "        env.step()\n",
   "        \n",
   "        # Получение результата\n",
   "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
   "        \n",
   "        done = len(terminal_steps) > 0\n",
   "        \n",
   "        if done:\n",
   "            # Эпизод завершен\n",
   "            reward = terminal_steps.reward[0]\n",
   "            next_state = terminal_steps.obs[0][0]\n",
   "        else:\n",
   "            # Эпизод продолжается\n",
   "            if len(decision_steps) > 0:\n",
   "                reward = decision_steps.reward[0]\n",
   "                next_state = decision_steps.obs[0][0]\n",
   "            else:\n",
   "                reward = 0\n",
   "                next_state = state\n",
   "        \n",
   "        # Сохранение опыта в буфер\n",
   "        replay_buffer.push(state, action, reward, next_state, done)\n",
   "        \n",
   "        episode_reward += reward\n",
   "        episode_length += 1\n",
   "        state = next_state\n",
   "        \n",
   "        # Обучение модели\n",
   "        if len(replay_buffer) > batch_size:\n",
   "            # Выборка из буфера\n",
   "            batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
   "            \n",
   "            # Преобразование в тензоры\n",
   "            states = torch.FloatTensor(batch_states).to(device)\n",
   "            actions = torch.FloatTensor(batch_actions).to(device)\n",
   "            rewards = torch.FloatTensor(batch_rewards).to(device)\n",
   "            next_states = torch.FloatTensor(batch_next_states).to(device)\n",
   "            dones = torch.BoolTensor(batch_dones).to(device)\n",
   "            \n",
   "            # Вычисление Q-значений\n",
   "            current_q_values = model(states)\n",
   "            next_q_values = target_model(next_states).detach()\n",
   "            \n",
   "            # Вычисление целевых значений\n",
   "            target_q_values = rewards.unsqueeze(1) + (gamma * next_q_values * (~dones).unsqueeze(1))\n",
   "            \n",
   "            # Вычисление потерь\n",
   "            loss = criterion(current_q_values, target_q_values)\n",
   "            \n",
   "            # Обратное распространение\n",
   "            optimizer.zero_grad()\n",
   "            loss.backward()\n",
   "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
   "            optimizer.step()\n",
   "            \n",
   "            losses.append(loss.item())\n",
   "        \n",
   "        if done:\n",
   "            break\n",
   "    \n",
   "    # Обновление целевой модели\n",
   "    if episode % target_update_freq == 0:\n",
   "        target_model.load_state_dict(model.state_dict())\n",
   "    \n",
   "    # Уменьшение epsilon\n",
   "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
   "    \n",
   "    # Сохранение статистики\n",
   "    episode_rewards.append(episode_reward)\n",
   "    episode_lengths.append(episode_length)\n",
   "    \n",
   "    # Вывод прогресса\n",
   "    if episode % 100 == 0:\n",
   "        avg_reward = np.mean(episode_rewards[-100:])\n",
   "        avg_length = np.mean(episode_lengths[-100:])\n",
   "        avg_loss = np.mean(losses[-100:]) if losses else 0\n",
   "        print(f\"Эпизод {episode}, Средняя награда: {avg_reward:.2f}, Средняя длина: {avg_length:.1f}, Потери: {avg_loss:.4f}, Epsilon: {epsilon:.3f}\")\n",
   "\n",
   "print(\"Обучение завершено!\")\n",
   "\n",
   "# Закрытие среды\n",
   "env.close()\n",
   "\n",
   "# Построение графиков\n",
   "plt.figure(figsize=(15, 5))\n",
   "\n",
   "plt.subplot(1, 3, 1)\n",
   "plt.plot(episode_rewards)\n",
   "plt.title('Награды по эпизодам')\n",
   "plt.xlabel('Эпизод')\n",
   "plt.ylabel('Награда')\n",
   "\n",
   "plt.subplot(1, 3, 2)\n",
   "plt.plot(episode_lengths)\n",
   "plt.title('Длина эпизодов')\n",
   "plt.xlabel('Эпизод')\n",
   "plt.ylabel('Шаги')\n",
   "\n",
   "plt.subplot(1, 3, 3)\n",
   "if losses:\n",
   "    plt.plot(losses)\n",
   "    plt.title('Потери обучения')\n",
   "    plt.xlabel('Шаг обучения')\n",
   "    plt.ylabel('Потери')\n",
   "\n",
   "plt.tight_layout()\n",
   "plt.show()\n",
   "\n",
   "# Сохранение модели в формате PyTorch\n",
   "torch.save(model.state_dict(), 'trained_agent_model.pth')\n",
   "print(\"Модель сохранена как 'trained_agent_model.pth'\")\n",
   "\n",
   "# Сохранение модели в формате ONNX\n",
   "model.eval()\n",
   "dummy_input = torch.randn(1, obs_size).to(device)\n",
   "\n",
   "try:\n",
   "    torch.onnx.export(\n",
   "        model,                          # модель\n",
   "        dummy_input,                    # пример входных данных\n",
   "        'trained_agent_model.onnx',     # имя файла\n",
   "        export_params=True,             # сохранить параметры модели\n",
   "        opset_version=11,               # версия ONNX opset\n",
   "        do_constant_folding=True,       # оптимизация констант\n",
   "        input_names=['observation'],    # имена входов\n",
   "        output_names=['action'],        # имена выходов\n",
   "        dynamic_axes={\n",
   "            'observation': {0: 'batch_size'},\n",
   "            'action': {0: 'batch_size'}\n",
   "        }\n",
   "    )\n",
   "    print(\"Модель успешно сохранена в формате ONNX как 'trained_agent_model.onnx'\")\n",
   "    \n",
   "    # Проверка ONNX модели\n",
   "    import onnx\n",
   "    onnx_model = onnx.load('trained_agent_model.onnx')\n",
   "    onnx.checker.check_model(onnx_model)\n",
   "    print(\"ONNX модель прошла проверку корректности\")\n",
   "    \n",
   "except Exception as e:\n",
   "    print(f\"Ошибка при сохранении в ONNX: {e}\")\n",
   "    print(\"Убедитесь, что установлен пакет onnx: pip install onnx\")\n",
   "\n",
   "# Сохранение статистики обучения\n",
   "import json\n",
   "training_stats = {\n",
   "    'episode_rewards': episode_rewards,\n",
   "    'episode_lengths': episode_lengths,\n",
   "    'losses': losses,\n",
   "    'hyperparameters': {\n",
   "        'num_episodes': num_episodes,\n",
   "        'batch_size': batch_size,\n",
   "        'gamma': gamma,\n",
   "        'epsilon_decay': epsilon_decay,\n",
   "        'learning_rate': 3e-4\n",
   "    }\n",
   "}\n",
   "\n",
   "with open('training_statistics.json', 'w') as f:\n",
   "    json.dump(training_stats, f, indent=2)\n",
   "\n",
   "print(\"Статистика обучения сохранена в 'training_statistics.json'\")\n",
   "print(f\"Финальная средняя награда за последние 100 эпизодов: {np.mean(episode_rewards[-100:]):.2f}\")"
  ]
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
