{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7813cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import os  # Работа с операционной системой (пути, файлы и т.д.)\n",
    "from mlagents_envs.environment import UnityEnvironment  # Основной класс для взаимодействия с Unity средой\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel  # Канал для настройки параметров среды (скорость, качество и т.д.)\n",
    "from mlagents_envs.registry import default_registry  # Реестр стандартных сред Unity (необязательно, если используете свою среду)\n",
    "import numpy as np  # Работа с массивами и матрицами\n",
    "import matplotlib.pyplot as plt  # Визуализация данных (графики, диаграммы)\n",
    "\n",
    "import torch  # Основная библиотека для работы с нейросетями и тензорами\n",
    "import torch.nn as nn  # Модуль для создания нейронных сетей\n",
    "import torch.optim as optim  # Оптимизаторы для обучения нейросетей\n",
    "import numpy as np  # Повторный импорт numpy (можно удалить, если выше уже импортировано)\n",
    "from mlagents_envs.environment import UnityEnvironment  # Повторный импорт UnityEnvironment (можно удалить, если выше уже импортировано)\n",
    "from mlagents_envs.base_env import ActionTuple  # Класс для упаковки действий агента"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e4c57",
   "metadata": {},
   "source": [
    "# Обучение агента в Unity ML-Agents\n",
    "В этом ноутбуке показан пример запуска обучения агента в пользовательской среде Unity, расположенной в этой папке. Для этого используется пакет `mlagents` и Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68d80c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доступные поведения: ['MyAgent?team=0']\n"
     ]
    }
   ],
   "source": [
    "# Путь к вашему Unity окружению (укажите путь к .exe, если экспортировано, либо используйте Editor для запуска из редактора)\n",
    "# env_path = None  # Если среда запускается из редактора Unity, иначе укажите путь к .exe\n",
    "\n",
    "env_path = os.path.join(os.getcwd(), r'N:\\MyRL\\My_First_NPC\\MyfirstMPC\\UnityEnvironment.exe')  # Замените на ваш путь к .exe\n",
    "\n",
    "\n",
    "engine_channel = EngineConfigurationChannel()  # Создаем канал для настройки параметров среды\n",
    "env = UnityEnvironment(file_name=env_path, side_channels=[engine_channel])  # Инициализируем среду Unity с указанным .exe и каналом\n",
    "env.reset()  # Сброс среды (начало нового эпизода)\n",
    "# Получение информации о среде\n",
    "behavior_names = list(env.behavior_specs.keys())  # Получаем список всех доступных поведений (behavior)\n",
    "print('Доступные поведения:', behavior_names)  # Выводим список доступных поведений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92cf748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация среды Unity\n",
    "# env = UnityEnvironment(file_name=\"path/to/your/environment\")  # Укажите путь к вашему файлу среды\n",
    "# env.reset()  # Сброс среды (начало нового эпизода)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "801f4aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпизод 1 завершён.\n"
     ]
    }
   ],
   "source": [
    "# Пример цикла взаимодействия с агентом\n",
    "behavior_name = behavior_names[0]  # Используем первое поведение из списка\n",
    "spec = env.behavior_specs[behavior_name]  # Получаем спецификацию поведения\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)  # Получаем текущие шаги агента (решения и терминальные)\n",
    "for episode in range(1):  # Запускаем один эпизод\n",
    "    env.reset()  # Сброс среды перед началом эпизода\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)  # Получаем шаги после сброса\n",
    "    while len(terminal_steps) == 0:  # Пока эпизод не завершён\n",
    "        action = spec.action_spec.random_action(len(decision_steps))  # Генерируем случайное действие для каждого агента\n",
    "        env.set_actions(behavior_name, action)  # Передаем действие в среду\n",
    "        env.step()  # Делаем шаг среды\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)  # Получаем новые шаги\n",
    "    print(f\"Эпизод {episode+1} завершён.\")  # Сообщаем о завершении эпизода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94a648a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение имени поведения\n",
    "behavior_names = list(env.behavior_specs.keys())  # Получаем список всех поведений\n",
    "behavior_name = behavior_names[0]  # Выбираем первое поведение\n",
    "spec = env.behavior_specs[behavior_name]  # Получаем спецификацию выбранного поведения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b441802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Имя поведения: MyAgent?team=0\n",
      "Спецификация поведения: BehaviorSpec(observation_specs=[ObservationSpec(shape=(6,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size6')], action_spec=ActionSpec(continuous_size=2, discrete_branches=(1,)))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Имя поведения: {behavior_name}\")  # Выводим имя выбранного поведения\n",
    "# Получение спецификации поведения\n",
    "print(f\"Спецификация поведения: {spec}\")  # Выводим спецификацию поведения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b633129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример простой нейросети для агента\n",
    "class SimpleAgentNet(nn.Module):\n",
    "    def __init__(self, obs_size, action_size):\n",
    "        super(SimpleAgentNet, self).__init__()  # Инициализация родительского класса\n",
    "        self.fc1 = nn.Linear(obs_size, 128)  # Первый полносвязный слой\n",
    "        self.fc2 = nn.Linear(128, 64)  # Второй полносвязный слой\n",
    "        self.fc3 = nn.Linear(64, action_size)  # Выходной слой, размер соответствует размеру действия\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x может быть (obs_size,) или (batch, obs_size)\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)  # Добавляем размер батча, если вход одномерный\n",
    "        x = torch.relu(self.fc1(x))  # Применяем ReLU к первому слою\n",
    "        x = torch.relu(self.fc2(x))  # Применяем ReLU ко второму слою\n",
    "        x = self.fc3(x)  # Выходной слой (логиты действий)\n",
    "        return x  # Возвращаем результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fa27cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер наблюдений: 6\n",
      "Размер действий: 2\n",
      "Начинаем обучение на 100000 эпизодов...\n"
     ]
    }
   ],
   "source": [
    "# Получаем размеры входа и выхода из спецификации поведения\n",
    "obs_size = spec.observation_specs[0].shape[0]  # Размерность наблюдения (входа в сеть)\n",
    "action_size = spec.action_spec.continuous_size  # Размерность непрерывного действия (выхода сети)\n",
    "\n",
    "# Инициализация сети, оптимизатора и функции потерь\n",
    "net = SimpleAgentNet(obs_size, action_size)  # Создаем экземпляр нейросети\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)  # Оптимизатор Adam для обновления весов сети\n",
    "loss_fn = nn.MSELoss()  # Функция потерь (среднеквадратичная ошибка)\n",
    "\n",
    "# Гиперпараметры\n",
    "num_episodes = 100000  # Количество эпизодов для обучения\n",
    "gamma = 0.99  # Коэффициент дисконтирования\n",
    "\n",
    "print(f\"Размер наблюдений: {obs_size}\")  # Выводим размер наблюдений\n",
    "print(f\"Размер действий: {action_size}\")  # Выводим размер действий\n",
    "print(f\"Начинаем обучение на {num_episodes} эпизодов...\")  # Сообщаем о начале обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a2cef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()  # Сброс среды перед обучением\n",
    "env.close()  # Закрытие среды после завершения обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67a6319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch  # Импортируем PyTorch для проверки доступности CUDA\n",
    "# Проверка доступности CUDA (GPU)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516f433",
   "metadata": {},
   "source": [
    "# Визуальное обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4c71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель успешно сохранена в формате ONNX: trained_agent.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch  # Импортируем PyTorch для работы с нейросетями\n",
    "import numpy as np  # Импортируем numpy для работы с массивами\n",
    "\n",
    "# Основной цикл обучения\n",
    "for episode in range(num_episodes):  # Для каждого эпизода\n",
    "    env.reset()  # Сброс среды\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)  # Получаем начальные шаги\n",
    "    episode_reward = 0  # Суммарная награда за эпизод\n",
    "    done = False  # Флаг завершения эпизода\n",
    "    \n",
    "    while not done:  # Пока эпизод не завершён\n",
    "        obs = decision_steps.obs[0]  # Получаем наблюдения агента\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32)  # Преобразуем наблюдения в тензор\n",
    "        \n",
    "        # Получаем действия от сети\n",
    "        action_tensor = net(obs_tensor)  # Прогоняем наблюдения через сеть\n",
    "        action_np = action_tensor.detach().numpy()  # Переводим результат в numpy-массив\n",
    "        \n",
    "        # Создаем ActionTuple с непрерывными и фиктивными дискретными действиями\n",
    "        action_tuple = ActionTuple()  # Создаем объект для хранения действий\n",
    "        action_tuple.add_continuous(action_np)  # Добавляем непрерывные действия\n",
    "        \n",
    "        # Добавляем фиктивные дискретные действия (нулевые)\n",
    "        discrete_action_size = spec.action_spec.discrete_size  # Размерность дискретных действий\n",
    "        if discrete_action_size > 0:\n",
    "            discrete_actions = np.zeros((len(decision_steps), discrete_action_size), dtype=np.int32)  # Массив нулей для дискретных действий\n",
    "            action_tuple.add_discrete(discrete_actions)  # Добавляем дискретные действия\n",
    "        \n",
    "        env.set_actions(behavior_name, action_tuple)  # Передаем действия в среду\n",
    "        env.step()  # Делаем шаг среды\n",
    "        \n",
    "        # Получаем новые шаги\n",
    "        next_decision_steps, next_terminal_steps = env.get_steps(behavior_name)  # Получаем новые шаги агента\n",
    "        \n",
    "        # Вычисляем награду и проверяем завершение эпизода\n",
    "        if len(next_terminal_steps) > 0:  # Если эпизод завершён\n",
    "            reward = next_terminal_steps.reward[0]  # Получаем награду за финальный шаг\n",
    "            done = True  # Завершаем эпизод\n",
    "        else:\n",
    "            reward = next_decision_steps.reward[0]  # Получаем награду за текущий шаг\n",
    "        \n",
    "        episode_reward += reward  # Суммируем награду\n",
    "        \n",
    "        # Обновляем сеть\n",
    "        target = torch.tensor([reward], dtype=torch.float32)  # Целевое значение для обучения\n",
    "        predicted = net(obs_tensor).mean(dim=1)  # Предсказание сети (усреднённое по батчу)\n",
    "        loss = loss_fn(predicted, target)  # Вычисляем ошибку\n",
    "        \n",
    "        optimizer.zero_grad()  # Обнуляем градиенты\n",
    "        loss.backward()  # Вычисляем градиенты\n",
    "        optimizer.step()  # Делаем шаг оптимизации\n",
    "        \n",
    "        decision_steps = next_decision_steps  # Переходим к следующим шагам\n",
    "    \n",
    "    if (episode + 1) % 100 == 0:  # Каждые 100 эпизодов выводим прогресс\n",
    "        print(f\"Эпизод {episode + 1}/{num_episodes}, награда: {episode_reward:.2f}\")\n",
    "\n",
    "# Сохранение итоговой модели в формате ONNX\n",
    "dummy_input = torch.randn(1, obs_size)  # Создаем фиктивный вход для экспорта\n",
    "torch.onnx.export(\n",
    "    net,  # Экспортируемая модель\n",
    "    dummy_input,  # Пример входа\n",
    "    \"trained_agent.onnx\",  # Имя выходного файла\n",
    "    input_names=['obs_0'],  # Имя входного тензора\n",
    "    output_names=['continuous_actions'],  # Имя выходного тензора\n",
    "    dynamic_axes={'obs_0': {0: 'batch_size'}, 'continuous_actions': {0: 'batch_size'}},  # Динамический размер батча\n",
    "    opset_version=9  # Версия ONNX\n",
    " )\n",
    "\n",
    "print(\"Модель успешно сохранена в формате ONNX: trained_agent.onnx\")  # Сообщаем об успешном сохранении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c6261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл существует: True\n"
     ]
    }
   ],
   "source": [
    "import os  # Импортируем модуль для работы с файлами\n",
    "print(\"Файл существует:\", os.path.exists(\"trained_agent.onnx\"))  # Проверяем, был ли успешно сохранён файл модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50cde6f",
   "metadata": {},
   "source": [
    "# Визуальное обучение на CUDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed4992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Импортируем PyTorch для работы с нейросетями и CUDA\n",
    "import numpy as np  # Импортируем numpy для работы с массивами\n",
    "\n",
    "# Проверяем доступность CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используемое устройство: {device}\")  # Сообщаем, на чём работаем\n",
    "\n",
    "# Переносим модель на устройство (GPU или CPU)\n",
    "net = SimpleAgentNet(obs_size, action_size).to(device)  # Инициализируем модель и сразу кидаем на GPU\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)  # Оптимизатор Adam\n",
    "loss_fn = nn.MSELoss()  # Функция потерь (среднеквадратичная ошибка)\n",
    "\n",
    "# Основной цикл обучения\n",
    "for episode in range(num_episodes):  # Для каждого эпизода\n",
    "    env.reset()  # Сброс среды\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)  # Получаем начальные шаги\n",
    "    episode_reward = 0  # Суммарная награда за эпизод\n",
    "    done = False  # Флаг завершения эпизода\n",
    "    \n",
    "    while not done:  # Пока эпизод не завершён\n",
    "        obs = decision_steps.obs[0]  # Получаем наблюдения агента\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)  # Преобразуем в тензор и кидаем на GPU\n",
    "        \n",
    "        # Получаем действия от сети\n",
    "        action_tensor = net(obs_tensor)  # Прогоняем наблюдения через сеть на GPU\n",
    "        action_np = action_tensor.detach().cpu().numpy()  # Возвращаем на CPU для работы с Unity\n",
    "        \n",
    "        # Создаем ActionTuple с непрерывными и фиктивными дискретными действиями\n",
    "        action_tuple = ActionTuple()  # Создаем объект для хранения действий\n",
    "        action_tuple.add_continuous(action_np)  # Добавляем непрерывные действия\n",
    "        \n",
    "        # Добавляем фиктивные дискретные действия (нулевые)\n",
    "        discrete_action_size = spec.action_spec.discrete_size  # Размерность дискретных действий\n",
    "        if discrete_action_size > 0:\n",
    "            discrete_actions = np.zeros((len(decision_steps), discrete_action_size), dtype=np.int32)  # Массив нулей\n",
    "            action_tuple.add_discrete(discrete_actions)  # Добавляем дискретные действия\n",
    "        \n",
    "        env.set_actions(behavior_name, action_tuple)  # Передаем действия в среду\n",
    "        env.step()  # Делаем шаг среды\n",
    "        \n",
    "        # Получаем новые шаги\n",
    "        next_decision_steps, next_terminal_steps = env.get_steps(behavior_name)  # Получаем новые шаги агента\n",
    "        \n",
    "        # Вычисляем награду и проверяем завершение эпизода\n",
    "        if len(next_terminal_steps) > 0:  # Если эпизод завершён\n",
    "            reward = next_terminal_steps.reward[0]  # Получаем награду за финальный шаг\n",
    "            done = True  # Завершаем эпизод\n",
    "        else:\n",
    "            reward = next_decision_steps.reward[0]  # Получаем награду за текущий шаг\n",
    "        \n",
    "        episode_reward += reward  # Суммируем награду\n",
    "        \n",
    "        # Обновляем сеть\n",
    "        target = torch.tensor([reward], dtype=torch.float32).to(device)  # Целевое значение на GPU\n",
    "        predicted = net(obs_tensor).mean(dim=1)  # Предсказание сети (усреднённое по батчу)\n",
    "        loss = loss_fn(predicted, target)  # Вычисляем ошибку\n",
    "        \n",
    "        optimizer.zero_grad()  # Обнуляем градиенты\n",
    "        loss.backward()  # Вычисляем градиенты\n",
    "        optimizer.step()  # Делаем шаг оптимизации\n",
    "        \n",
    "        decision_steps = next_decision_steps  # Переходим к следующим шагам\n",
    "    \n",
    "    if (episode + 1) % 100 == 0:  # Каждые 100 эпизодов выводим прогресс\n",
    "        print(f\"Эпизод {episode + 1}/{num_episodes}, награда: {episode_reward:.2f}\")\n",
    "\n",
    "# Сохранение итоговой модели в формате ONNX\n",
    "dummy_input = torch.randn(1, obs_size).to(device)  # Фиктивный вход на GPU\n",
    "torch.onnx.export(\n",
    "    net,  # Экспортируемая модель\n",
    "    dummy_input,  # Пример входа\n",
    "    \"trained_agent.onnx\",  # Имя выходного файла\n",
    "    input_names=['obs_0'],  # Имя входного тензора\n",
    "    output_names=['continuous_actions'],  # Имя выходного тензора\n",
    "    dynamic_axes={'obs_0': {0: 'batch_size'}, 'continuous_actions': {0: 'batch_size'}},  # Динамический размер батча\n",
    "    opset_version=9  # Версия ONNX\n",
    ")\n",
    "\n",
    "print(\"Модель успешно сохранена в формате ONNX: trained_agent.onnx\")  # Сообщаем об успешном сохранении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a32cc553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cuda\n",
      "Эпизод 1: Кол-во агентов: 1, Форма наблюдений: (1, 6)\n",
      "Эпизод 1/10, Награда: -7.32, Время: 0.56 сек\n",
      "Эпизод 2: Кол-во агентов: 1, Форма наблюдений: (1, 6)\n",
      "Эпизод 2/10, Награда: -7.08, Время: 0.68 сек\n",
      "Эпизод 3: Кол-во агентов: 1, Форма наблюдений: (1, 6)\n",
      "Эпизод 3/10, Награда: -6.38, Время: 0.58 сек\n",
      "Эпизод 4: Кол-во агентов: 1, Форма наблюдений: (1, 6)\n",
      "Эпизод 4/10, Награда: -7.54, Время: 0.81 сек\n",
      "Эпизод 5: Кол-во агентов: 1, Форма наблюдений: (1, 6)\n",
      "Эпизод 5/10, Награда: -5.98, Время: 0.49 сек\n",
      "Модель успешно сохранена в формате ONNX с метаданными\n"
     ]
    }
   ],
   "source": [
    "import torch  # PyTorch для нейронок и CUDA\n",
    "import numpy as np  # Для работы с массивами\n",
    "import time  # Для замера времени эпизодов\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "\n",
    "# Настройка канала для ускорения Unity\n",
    "engine_channel = EngineConfigurationChannel()\n",
    "engine_channel.set_configuration_parameters(\n",
    "    time_scale=50.0,  # Ускоряем симуляцию в 50 раз (попробуй 20, 50 или 100, если нужно ещё быстрее)\n",
    "    target_frame_rate=60,  # Ограничиваем FPS для стабильности\n",
    "    quality_level=0  # Минимальное качество графики для снижения нагрузки\n",
    ")\n",
    "\n",
    "# Инициализация среды Unity с рендерингом\n",
    "env_path = os.path.join(os.getcwd(), r'N:\\MyRL\\My_First_NPC\\MyfirstMPC\\UnityEnvironment.exe')\n",
    "env = UnityEnvironment(\n",
    "    file_name=env_path,\n",
    "    side_channels=[engine_channel],\n",
    "    no_graphics=False  # Включаем рендеринг для визуального кайфа\n",
    ")\n",
    "env.reset()\n",
    "\n",
    "# Проверяем CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используемое устройство: {device}\")\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"CUDA недоступна, обучение будет на CPU!\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Переносим модель на устройство\n",
    "net = SimpleAgentNet(obs_size, action_size).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Основной цикл обучения\n",
    "try:\n",
    "    for episode in range(5):\n",
    "        env.reset()\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        start_time = time.time()  # Замеряем время эпизода\n",
    "\n",
    "        print(f\"Эпизод {episode + 1}: Кол-во агентов: {len(decision_steps)}, Форма наблюдений: {decision_steps.obs[0].shape}\")\n",
    "\n",
    "        while not done:\n",
    "            obs = decision_steps.obs[0]  # Наблюдения всех агентов\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)  # На GPU\n",
    "\n",
    "            # Получаем действия\n",
    "            action_tensor = net(obs_tensor)\n",
    "            action_np = action_tensor.detach().cpu().numpy()\n",
    "\n",
    "            # ActionTuple для всех агентов\n",
    "            action_tuple = ActionTuple()\n",
    "            action_tuple.add_continuous(action_np)\n",
    "\n",
    "            # Добавляем фиктивные дискретные действия\n",
    "            discrete_action_size = spec.action_spec.discrete_size\n",
    "            if discrete_action_size > 0:\n",
    "                discrete_actions = np.zeros((len(decision_steps), discrete_action_size), dtype=np.int32)\n",
    "                action_tuple.add_discrete(discrete_actions)\n",
    "\n",
    "            env.set_actions(behavior_name, action_tuple)\n",
    "            env.step()\n",
    "\n",
    "            # Получаем новые шаги\n",
    "            next_decision_steps, next_terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "            # Вычисляем награду и проверяем завершение\n",
    "            if len(next_terminal_steps) > 0:\n",
    "                reward = next_terminal_steps.reward[0]\n",
    "                done = True\n",
    "            else:\n",
    "                reward = next_decision_steps.reward[0]\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Обновляем сеть\n",
    "            target = torch.tensor([reward], dtype=torch.float32).to(device)\n",
    "            predicted = net(obs_tensor).mean(dim=1)\n",
    "            loss = loss_fn(predicted, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            decision_steps = next_decision_steps\n",
    "\n",
    "        episode_time = time.time() - start_time\n",
    "        print(f\"Эпизод {episode + 1}/{num_episodes}, Награда: {episode_reward:.2f}, Время: {episode_time:.2f} сек\")\n",
    "\n",
    "    # Сохранение модели (как в предыдущем ответе)\n",
    "    dummy_input = torch.randn(1, obs_size).to(device)\n",
    "    torch.onnx.export(\n",
    "        net,\n",
    "        dummy_input,\n",
    "        \"trained_agent.onnx\",\n",
    "        input_names=['obs_0'],\n",
    "        output_names=['continuous_actions'],\n",
    "        dynamic_axes={'obs_0': {0: 'batch_size'}, 'continuous_actions': {0: 'batch_size'}},\n",
    "        opset_version=9,\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "\n",
    "    # Добавляем метаданные вручную\n",
    "    import onnx\n",
    "    from onnx import helper\n",
    "\n",
    "    model = onnx.load(\"trained_agent.onnx\")\n",
    "    metadata_props = {\n",
    "        \"version_number\": \"1.2.0\",\n",
    "        \"behavior_name\": behavior_name,\n",
    "        \"action_size\": str(action_size),\n",
    "        \"observation_size\": str(obs_size),\n",
    "        \"discrete_branches\": str(spec.action_spec.discrete_branches),\n",
    "        \"continuous_size\": str(spec.action_spec.continuous_size)\n",
    "    }\n",
    "\n",
    "    for key, value in metadata_props.items():\n",
    "        meta = model.metadata_props.add()\n",
    "        meta.key = key\n",
    "        meta.value = value\n",
    "\n",
    "    onnx.save(model, \"trained_agent.onnx\")\n",
    "    print(\"Модель успешно сохранена в формате ONNX с метаданными\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка в цикле обучения: {e}\")\n",
    "    raise e\n",
    "\n",
    "finally:\n",
    "    env.close()  # Закрываем среду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaba6148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка модели: trained_agent.onnx\n",
      "Добавляемые метаданные:\n",
      "  version_number: 3.0.0\n",
      "  action_spec_version: 2\n",
      "  behavior_name: MyAgent\n",
      "  is_continuous_control: True\n",
      "  is_discrete_control: True\n",
      "  action_output_shape: (3,)\n",
      "  observation_space_size: 6\n",
      "  memory_size: 0\n",
      "\n",
      "Модель успешно сохранена в 'trained_agent.onnx' со всеми необходимыми метаданными.\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# --- ШАГ 1: Определяем точные параметры из вашего скриншота ---\n",
    "\n",
    "# Путь к вашей модели\n",
    "model_path = \"trained_agent.onnx\" \n",
    "# Имя поведения из Unity\n",
    "behavior_name = \"MyAgent\" \n",
    "# Space Size из Vector Observation\n",
    "observation_size = 6 \n",
    "# Continuous Actions\n",
    "continuous_action_size = 2 \n",
    "# Discrete Branch Sizes (у вас одна ветка размером 1)\n",
    "discrete_branch_sizes = (1,) \n",
    "\n",
    "# --- ШАГ 2: Запускаем скрипт для добавления ВСЕХ метаданных ---\n",
    "\n",
    "print(f\"Загрузка модели: {model_path}\")\n",
    "model = onnx.load(model_path)\n",
    "\n",
    "# Полный набор метаданных, необходимый для ML-Agents\n",
    "# \"version_number\": \"3.0.0\" - это стандартное значение для современных версий ML-Agents.\n",
    "# \"action_spec_version\": \"2\" - также стандартное значение.\n",
    "metadata = {\n",
    "    \"version_number\": \"3.0.0\",\n",
    "    \"action_spec_version\": \"2\",\n",
    "    \"behavior_name\": behavior_name,\n",
    "    \"is_continuous_control\": \"True\" if continuous_action_size > 0 else \"False\",\n",
    "    \"is_discrete_control\": \"True\" if len(discrete_branch_sizes) > 0 and discrete_branch_sizes[0] > 0 else \"False\",\n",
    "    # Формируем строку для action_output_shape. Например: (2, 1) -> \"(2,1)\"\n",
    "    \"action_output_shape\": str( (continuous_action_size + sum(discrete_branch_sizes),) ).replace(\" \", \"\"),\n",
    "    # Другие важные поля\n",
    "    \"observation_space_size\": str(observation_size),\n",
    "    \"memory_size\": \"0\" # Установите другое значение, если используете LSTM (память)\n",
    "}\n",
    "\n",
    "print(\"Добавляемые метаданные:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    # Удаляем старый ключ, если он существует, чтобы избежать дубликатов\n",
    "    for i in reversed(range(len(model.metadata_props))):\n",
    "        if model.metadata_props[i].key == key:\n",
    "            del model.metadata_props[i]\n",
    "\n",
    "    # Добавляем новый ключ\n",
    "    meta = model.metadata_props.add()\n",
    "    meta.key = key\n",
    "    meta.value = value\n",
    "\n",
    "# Сохраняем обновленную модель\n",
    "onnx.save(model, model_path)\n",
    "print(f\"\\nМодель успешно сохранена в '{model_path}' со всеми необходимыми метаданными.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f517ad3",
   "metadata": {},
   "source": [
    "# Обучение без рендеринга видео. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
